{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a44b7e",
   "metadata": {},
   "source": [
    "# Single Neuron Logistic Regression\n",
    "## In this notebook, I will build a Single Neuron Logistic Regression model to predict if a player will last 5 years in league."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a2680d",
   "metadata": {},
   "source": [
    "### Load necessary libraries and the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sns.set_theme()\n",
    "df = pd.read_csv(\"nba_logreg.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49685afe",
   "metadata": {},
   "source": [
    "### Then, let's check our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5adc28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GP</th>\n",
       "      <th>MIN</th>\n",
       "      <th>PTS</th>\n",
       "      <th>FGM</th>\n",
       "      <th>FGA</th>\n",
       "      <th>FG%</th>\n",
       "      <th>3P Made</th>\n",
       "      <th>3PA</th>\n",
       "      <th>3P%</th>\n",
       "      <th>FTM</th>\n",
       "      <th>FTA</th>\n",
       "      <th>FT%</th>\n",
       "      <th>OREB</th>\n",
       "      <th>DREB</th>\n",
       "      <th>REB</th>\n",
       "      <th>AST</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>TOV</th>\n",
       "      <th>TARGET_5Yrs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "      <td>1340.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>60.414179</td>\n",
       "      <td>17.624627</td>\n",
       "      <td>6.801493</td>\n",
       "      <td>2.629104</td>\n",
       "      <td>5.885299</td>\n",
       "      <td>44.169403</td>\n",
       "      <td>0.247612</td>\n",
       "      <td>0.779179</td>\n",
       "      <td>19.308126</td>\n",
       "      <td>1.297687</td>\n",
       "      <td>1.821940</td>\n",
       "      <td>70.300299</td>\n",
       "      <td>1.009403</td>\n",
       "      <td>2.025746</td>\n",
       "      <td>3.034478</td>\n",
       "      <td>1.550522</td>\n",
       "      <td>0.618507</td>\n",
       "      <td>0.368582</td>\n",
       "      <td>1.193582</td>\n",
       "      <td>0.620149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.433992</td>\n",
       "      <td>8.307964</td>\n",
       "      <td>4.357545</td>\n",
       "      <td>1.683555</td>\n",
       "      <td>3.593488</td>\n",
       "      <td>6.137679</td>\n",
       "      <td>0.383688</td>\n",
       "      <td>1.061847</td>\n",
       "      <td>16.022916</td>\n",
       "      <td>0.987246</td>\n",
       "      <td>1.322984</td>\n",
       "      <td>10.578479</td>\n",
       "      <td>0.777119</td>\n",
       "      <td>1.360008</td>\n",
       "      <td>2.057774</td>\n",
       "      <td>1.471169</td>\n",
       "      <td>0.409759</td>\n",
       "      <td>0.429049</td>\n",
       "      <td>0.722541</td>\n",
       "      <td>0.485531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>23.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>10.875000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>40.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>64.700000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>5.550000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>44.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>71.250000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>47.900000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>77.600000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>82.000000</td>\n",
       "      <td>40.900000</td>\n",
       "      <td>28.200000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>19.800000</td>\n",
       "      <td>73.700000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>13.900000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                GP          MIN          PTS          FGM          FGA  \\\n",
       "count  1340.000000  1340.000000  1340.000000  1340.000000  1340.000000   \n",
       "mean     60.414179    17.624627     6.801493     2.629104     5.885299   \n",
       "std      17.433992     8.307964     4.357545     1.683555     3.593488   \n",
       "min      11.000000     3.100000     0.700000     0.300000     0.800000   \n",
       "25%      47.000000    10.875000     3.700000     1.400000     3.300000   \n",
       "50%      63.000000    16.100000     5.550000     2.100000     4.800000   \n",
       "75%      77.000000    22.900000     8.800000     3.400000     7.500000   \n",
       "max      82.000000    40.900000    28.200000    10.200000    19.800000   \n",
       "\n",
       "               FG%      3P Made          3PA          3P%          FTM  \\\n",
       "count  1340.000000  1340.000000  1340.000000  1329.000000  1340.000000   \n",
       "mean     44.169403     0.247612     0.779179    19.308126     1.297687   \n",
       "std       6.137679     0.383688     1.061847    16.022916     0.987246   \n",
       "min      23.800000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%      40.200000     0.000000     0.000000     0.000000     0.600000   \n",
       "50%      44.100000     0.100000     0.300000    22.400000     1.000000   \n",
       "75%      47.900000     0.400000     1.200000    32.500000     1.600000   \n",
       "max      73.700000     2.300000     6.500000   100.000000     7.700000   \n",
       "\n",
       "               FTA          FT%         OREB         DREB          REB  \\\n",
       "count  1340.000000  1340.000000  1340.000000  1340.000000  1340.000000   \n",
       "mean      1.821940    70.300299     1.009403     2.025746     3.034478   \n",
       "std       1.322984    10.578479     0.777119     1.360008     2.057774   \n",
       "min       0.000000     0.000000     0.000000     0.200000     0.300000   \n",
       "25%       0.900000    64.700000     0.400000     1.000000     1.500000   \n",
       "50%       1.500000    71.250000     0.800000     1.700000     2.500000   \n",
       "75%       2.300000    77.600000     1.400000     2.600000     4.000000   \n",
       "max      10.200000   100.000000     5.300000     9.600000    13.900000   \n",
       "\n",
       "               AST          STL          BLK          TOV  TARGET_5Yrs  \n",
       "count  1340.000000  1340.000000  1340.000000  1340.000000  1340.000000  \n",
       "mean      1.550522     0.618507     0.368582     1.193582     0.620149  \n",
       "std       1.471169     0.409759     0.429049     0.722541     0.485531  \n",
       "min       0.000000     0.000000     0.000000     0.100000     0.000000  \n",
       "25%       0.600000     0.300000     0.100000     0.700000     0.000000  \n",
       "50%       1.100000     0.500000     0.200000     1.000000     1.000000  \n",
       "75%       2.000000     0.800000     0.500000     1.500000     1.000000  \n",
       "max      10.600000     2.500000     3.900000     4.400000     1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b52b4796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1340, 21)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c1faf3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name            0\n",
       "GP              0\n",
       "MIN             0\n",
       "PTS             0\n",
       "FGM             0\n",
       "FGA             0\n",
       "FG%             0\n",
       "3P Made         0\n",
       "3PA             0\n",
       "3P%            11\n",
       "FTM             0\n",
       "FTA             0\n",
       "FT%             0\n",
       "OREB            0\n",
       "DREB            0\n",
       "REB             0\n",
       "AST             0\n",
       "STL             0\n",
       "BLK             0\n",
       "TOV             0\n",
       "TARGET_5Yrs     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c3b51",
   "metadata": {},
   "source": [
    "### According to the result, there are 21 variables in the dataset. Besides, there are 11 missing values in '3P%' variable. Just because there are total 1340 data points in the dataset, 11 missing values can be left out. It will not cause big problem for building models. Also, I delete 'name' variable for it is a string which will not contribute to the label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2110e352",
   "metadata": {},
   "source": [
    "### Define X and y, seperate the dataset as training dataset and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f33cd6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1329, 19)\n",
      "(1329,)\n"
     ]
    }
   ],
   "source": [
    "df=df.drop(df[df['3P%'].isnull()].index)\n",
    "y = df['TARGET_5Yrs']\n",
    "columns_to_drop = ['TARGET_5Yrs','Name']\n",
    "X = df.drop(columns=columns_to_drop)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "241dfe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,  random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cfe1cb",
   "metadata": {},
   "source": [
    "### After dealing with the missing values, we can begin with building the Single Neuron Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a22dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "class SingleNeuron(object):\n",
    "    \"\"\"\n",
    "    A class used to represent a single artificial neuron. \n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    activation_function : function\n",
    "        The activation function applied to the preactivation linear combination.\n",
    "    \n",
    "    cost_function : function\n",
    "        The cost function used to measure model performance.\n",
    "\n",
    "    w_ : numpy.ndarray\n",
    "        The weights and bias of the single neuron. The last entry being the bias. \n",
    "        This attribute is created when the train method is called.\n",
    "\n",
    "    errors_: list\n",
    "        A list containing the mean sqaured error computed after each iteration \n",
    "        of stochastic gradient descent per epoch. \n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    train(self, X, y, alpha = 0.005, epochs = 50)\n",
    "        Iterates the stochastic gradient descent algorithm through each sample \n",
    "        a total of epochs number of times with learning rate alpha. The data \n",
    "        used consists of feature vectors X and associated labels y. \n",
    "\n",
    "    predict(self, X)\n",
    "        Uses the weights and bias, the feature vectors in X, and the \n",
    "        activation_function to make a y_hat prediction on each feature vector. \n",
    "    \"\"\"\n",
    "    def __init__(self, activation_function, cost_function):\n",
    "        self.activation_function = activation_function\n",
    "        self.cost_function = cost_function\n",
    "\n",
    "    def train(self, X, y, alpha = 0.005, epochs = 50):\n",
    "        self.w_ = np.random.rand(1 + X.shape[1])\n",
    "        self.errors_ = []\n",
    "        N = X.shape[0]\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                error = (self.predict(xi) - target)\n",
    "                self.w_[:-1] -= alpha*error*xi\n",
    "                self.w_[-1] -= alpha*error\n",
    "                #errors += .5*((self.predict(xi) - target)**2)\n",
    "                errors += self.cost_function(self.predict(xi), target)\n",
    "            self.errors_.append(errors/N)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        preactivation = np.dot(X, self.w_[:-1]) + self.w_[-1]\n",
    "        return self.activation_function(preactivation)\n",
    "\n",
    "    def plot_cost_function(self):\n",
    "        fig, axs = plt.subplots(figsize = (10, 8))\n",
    "        axs.plot(range(1, len(self.errors_) + 1), \n",
    "                self.errors_,\n",
    "                label = \"Cost function\")\n",
    "        axs.set_xlabel(\"epochs\", fontsize = 15)\n",
    "        axs.set_ylabel(\"Cost\", fontsize = 15)\n",
    "        axs.legend(fontsize = 15)\n",
    "        axs.set_title(\"Cost Calculated after Epoch During Training\", fontsize = 18)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_decision_boundary(self, X, y, xstring=\"x\", ystring=\"y\"):\n",
    "        plt.figure(figsize = (10, 8))\n",
    "        plot_decision_regions(X, y, clf = self)\n",
    "        plt.title(\"Neuron Decision Boundary\", fontsize = 18)\n",
    "        plt.xlabel(xstring, fontsize = 15)\n",
    "        plt.ylabel(ystring, fontsize = 15)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b376e809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/w3cqlkc16gx52552_9pn4hzh0000gn/T/ipykernel_8387/2463744315.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return - y*np.log(y_hat) - (1 - y)*np.log(1 - y_hat)\n",
      "/var/folders/ss/w3cqlkc16gx52552_9pn4hzh0000gn/T/ipykernel_8387/2463744315.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return - y*np.log(y_hat) - (1 - y)*np.log(1 - y_hat)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.SingleNeuron at 0x7fb82958f3a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_loss(y_hat, y):\n",
    "    return - y*np.log(y_hat) - (1 - y)*np.log(1 - y_hat)\n",
    "\n",
    "node = SingleNeuron(sigmoid, cross_entropy_loss)\n",
    "X_train=X_train.astype(float)\n",
    "X_train=X_train.values\n",
    "y_train=y_train.astype(float)\n",
    "y_train=y_train.values\n",
    "node.train(X_train, y_train, alpha = 0.01, epochs = 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d7f992c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Error of training dataset = 0.4835371589840075\n",
      "Classification Error of testing dataset = 0.5037593984962406\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.rint(node.predict(X_train)) != y_train\n",
    "classification_error = (np.rint(node.predict(X_train)) != y_train).astype(int)\n",
    "print(f\"Classification Error of training dataset = {sum(classification_error)/ len(y_train)}\")\n",
    "X_test=X_test.astype(float)\n",
    "X_test=X_test.values\n",
    "y_test=y_test.astype(float)\n",
    "y_test=y_test.values\n",
    "classification_error1 = (np.rint(node.predict(X_test)) != y_test).astype(int)\n",
    "print(f\"Classification Error of testing dataset = {sum(classification_error1)/ len(y_test)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b84b0e",
   "metadata": {},
   "source": [
    "### According to the result, the classification error of training dataset is about 48%. The classification Error of testing dataset is about 50%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
